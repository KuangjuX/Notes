# Sparse Attention

## Sparse Attention

ç”±äº Self-Attention ä¸­çš„è®¡ç®—å¤æ‚åº¦æ˜¯ $O(n^2)$ï¼Œå› æ­¤å†…å­˜å’Œè®¡ç®—çš„è´Ÿè½½å¾ˆå¤§ã€‚åŸå› åœ¨äº Self-Attention ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½è·Ÿåºåˆ—ä¸­çš„æ‰€æœ‰å…ƒç´ å…³è”ã€‚è¦èŠ‚çœæ˜¾å­˜ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œä¸€ä¸ªåŸºæœ¬çš„æ€è·¯å°±æ˜¯å‡å°‘å…³è”æ€§çš„è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯è®¤ä¸ºæ¯ä¸ªå…ƒç´ åªè·Ÿåºåˆ—å†…çš„ä¸€éƒ¨åˆ†å…ƒç´ ç›¸å…³ï¼Œè¿™å°±æ˜¯ **Sparse Attention çš„åŸºæœ¬åŸç†**ã€‚

### Atrous Self Attention

ä¸­æ–‡å«â€œè†¨èƒ€è‡ªæ³¨æ„åŠ›â€ï¼Œâ€œç©ºæ´è‡ªæ³¨æ„åŠ›â€ï¼Œå®ƒå¯¹ç›¸å…³æ€§è¿›è¡Œäº†çº¦æŸï¼Œå¼ºè¡Œè¦æ±‚æ¯ä¸ªå…ƒç´ åªè·Ÿå®ƒç›¸å¯¹è·ç¦»ä¸º k, 2k, 3k, ..., nk çš„å…ƒç´ å…³è”ï¼Œå…¶ä¸­ k æ˜¯é¢„å…ˆè®¾å®šçš„è¶…å‚æ•°ã€‚

![](SparseAttention/fig1.png)

æ¯ä¸ªå…ƒç´ åªè·Ÿå¤§çº¦ $ n/k $ ä¸ªå…ƒç´ ç›¸å…³ï¼Œè¿™æ ·è¿è¡Œæ•ˆç‡å’Œæ˜¾å­˜å ç”¨éƒ½å˜æˆäº† $ O(n^2 / k) $ï¼Œä¹Ÿå°±æ˜¯è¯´èƒ½ç›´æ¥é™ä½åˆ°åŸæ¥çš„ $ 1/k $ã€‚

### Local Self Attention

ä¸­æ–‡å«åš â€œå±€éƒ¨è‡ªæ³¨æ„åŠ›â€ã€‚çº¦æŸæ¯ä¸ªå…ƒç´ åªä¸å‰å k ä¸ªå…ƒç´ ä»¥åŠè‡ªèº«æœ‰å…³ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](SparseAttention/fig2.png)

Local Self Attention ä¿ç•™ä¸€ä¸ª $ 2k+1 $ å¤§å°çš„æ»‘åŠ¨çª—å£ï¼Œç„¶ååªä¿ç•™çª—å£å†…çš„è‡ªæ³¨æ„åŠ›ã€‚æ¯ä¸ªå…ƒç´ åªå’Œ $ 2k+1 $ ä¸ªå…ƒç´ æœ‰å…³ï¼Œè¿™æ ·æ˜¾å­˜å ç”¨å’Œè®¡ç®—é‡å˜ä¸º $ O((2k+1)n) ~ O(kn) $ã€‚ä½†æ˜¯ç›´æ¥ç‰ºç‰²äº†é•¿ç¨‹å…³è”æ€§ã€‚

### Sparse Attention

è¿™é‡ŒæŒ‡çš„æ˜¯ OpenAI çš„ Sparse Attention: ã€ŠGenerating Long Sequences with Sparse Transformersã€‹ã€‚Atrous Self Attention æ˜¯å¸¦æœ‰ä¸€äº›æ´çš„ï¼Œè€Œ Local Self Attention ç”¨æ¥å¡«è¡¥è¿™äº›æ´ã€‚OpenAI ç›´æ¥å°†ä¸¤ä¸ª Atrous Self Attention å’Œ Local Self Attention åˆå¹¶ä¸ºä¸€ä¸ªã€‚

![](SparseAttention/fig3.png)


## Big Bird

BigBird æ‰€å…³æ³¨çš„é—®é¢˜ï¼š

- èƒ½å¦ä½¿ç”¨æ›´å°‘çš„å†…ç§¯æ¥å®Œæˆå®Œå…¨è‡ªæ³¨æ„åŠ›çš„ç»éªŒä¼˜åŠ¿ï¼Ÿ
- è¿™äº›ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ˜¯å¦ä¿ç•™äº†åŸå§‹ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›å’Œçµæ´»æ€§ï¼Ÿ

### BigBird Architecture

![](SparseAttention/fig4.png)

BigBird å¯ä»¥ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šRandom Attention + Window Attention + Global Attentionã€‚

å¯¹äº Global Attentionï¼Œæ¯ä¸€ä¸ª query éƒ½å’Œå…¶ä»–æ‰€æœ‰ tokens å…³è”ï¼Œå¹¶ä¸”ä¹Ÿè¢«å…¶ä»–æ‰€æœ‰ tokens å…³è”ã€‚

```
# pseudo code

Q -> Query martix (seq_length, head_dim)
K -> Key matrix (seq_length, head_dim)

# 1st & last token attends all other tokens
Q[0] x [K[0], K[1], K[2], ......, K[n-1]]
Q[n-1] x [K[0], K[1], K[2], ......, K[n-1]]

# 1st & last token getting attended by all other tokens
K[0] x [Q[0], Q[1], Q[2], ......, Q[n-1]]
K[n-1] x [Q[0], Q[1], Q[2], ......, Q[n-1]]

```

å¯¹äº Sliding Attentionï¼Œå…³é”®è¯å…ƒè¢«æ‹·è´ä¸¤æ¬¡ï¼Œå‘å³æ‹·è´ä¸€æ¬¡ï¼Œå‘å·¦æ‹·è´ä¸€æ¬¡ï¼Œè®¡ç®—å¤æ‚åº¦ä¸º $ O(3 \times n) $ã€‚

```
# what we want to do
Q[i] x [K[i-1], K[i], K[i+1]] for i = 1:-1

# efficient implementation in code (assume dot product multiplication ğŸ‘‡)
[Q[0], Q[1], Q[2], ......, Q[n-2], Q[n-1]] x [K[1], K[2], K[3], ......, K[n-1], K[0]]
[Q[0], Q[1], Q[2], ......, Q[n-1]] x [K[n-1], K[0], K[1], ......, K[n-2]]
[Q[0], Q[1], Q[2], ......, Q[n-1]] x [K[0], K[1], K[2], ......, K[n-1]]

# Each sequence is getting multiplied by only 3 sequences to keep `window_size = 3`.
# Some computations might be missing; this is just a rough idea.
```

æ¯ä¸ªæŸ¥è¯¢æ ‡è®°ä¼šå…³æ³¨ä¸€äº›éšæœºæ ‡è®°ï¼Œå¯¹äºå®é™…å®ç°ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ä¼šéšæœºæ”¶é›†ä¸€äº›æ ‡è®°å¹¶è®¡ç®—å®ƒä»¬çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚

```
# r1, r2, r are some random indices; Note: r1, r2, r3 are different for each row ğŸ‘‡
Q[1] x [Q[r1], Q[r2], ......, Q[r]]
.
.
.
Q[n-2] x [Q[r1], Q[r2], ......, Q[r]]

# leaving 0th & (n-1)th token since they are already global

```

## References

- ä¸ºèŠ‚çº¦è€Œç”Ÿï¼šä»æ ‡å‡†Attentionåˆ°ç¨€ç–Attention, https://spaces.ac.cn/archives/6853#Sparse%20Self%20Attention
-  Big Bird: Transformers for Longer Sequences, https://arxiv.org/abs/2007.14062
- Understanding BigBird's Block Sparse Attention, https://huggingface.co/blog/big-bird